{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__check_build' from 'sklearn' (C:\\local\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b62fddfd7db4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '__check_build' from 'sklearn' (C:\\local\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "data['Sex']\n",
    "male, female=list(data.Sex).count('male'),list(data.Sex).count('female')\n",
    "\n",
    "male_survived=list(data.Sex[data.Survived==1]).count('male')\n",
    "female_survived=list(data.Sex[data.Survived==1]).count('female')\n",
    "print(\"Всего мужчин: \",male,\"Всего женщин: \", female)\n",
    "print(\"Выжило мужчин: \",male_survived,\"Выжило женщин: \",female_survived)\n",
    "print(\"Выжило мужчин в %: \",male_survived/male*100,\"Выжило женщин в %: \",female_survived/female*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Из-за команды грузиться сначала женщинам их выжило больше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived , deceased =list(data.Survived).count(1),list(data.Survived).count(0)\n",
    "print(' Выжившие= ',survived ,'\\n','Умершие = ',deceased )\n",
    "\n",
    "#Распределение возраста\n",
    "age_distribution=data.hist(column=['Age'], bins=40, figsize=(16,8),label=\"Распределение возраста\")\n",
    "plt.title('Распределение возраста')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "age_mean_survived=data[(data['Survived']==1)][\"Age\"].mean()\n",
    "age_median_survived=data[(data['Survived']==1)][\"Age\"].median()\n",
    "age_mean_deceased=data[(data['Survived']==0)][\"Age\"].mean()\n",
    "age_median_deceased=data[(data['Survived']==0)][\"Age\"].median()\n",
    "\n",
    "print(\"Age_mean выживших = \",age_mean_survived)\n",
    "print(\"Age_median выживших = \",age_median_survived)\n",
    "print(\"Age_mean умерших = \",age_mean_deceased)\n",
    "print(\"Age_median умерших = \",age_median_deceased)\n",
    "\n",
    "#распределение возраста погибших и выживших\n",
    "survived=data[(data['Survived']==1)][\"Age\"]\n",
    "survived.hist(bins=40, figsize=(16,8), alpha=0.5,label=\"Выжившие люди\")\n",
    "deceased=data[(data['Survived']==0)][\"Age\"]\n",
    "deceased.hist(bins=40, figsize=(16,8),alpha=0.6, label=\"Умершие люди\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#молодых грузили в шлюпки в последнюю очередь, поэтому и погибло больше всего людей от 16 до 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#каюты первого класса были ближе к первой палубе + пассажиров первого класса начали раньше сажать в шлюпки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-958c3b29ec25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# распределение пассажиров по классам\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclass1_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclass2_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclass3_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclass1_survived\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPclass\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSurvived\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# распределение пассажиров по классам\n",
    "class1_all=list(data.Pclass).count(1)\n",
    "class2_all=list(data.Pclass).count(2)\n",
    "class3_all=list(data.Pclass).count(3)\n",
    "class1_survived=list(data.Pclass[data.Survived==1]).count(1)\n",
    "class2_survived=list(data.Pclass[data.Survived==1]).count(2)\n",
    "class3_survived=list(data.Pclass[data.Survived==1]).count(3)\n",
    "\n",
    "# Распределение пассажиров по классам в %\n",
    "print(\"1 класс выжившие: \", class1_survived, \" from \",class1_all, \" ( \",class1_survived/class1_all*100,\" %)\" )\n",
    "print(\"2 класс выжившие: \", class2_survived, \" from \",class2_all, \" ( \",class2_survived/class2_all*100,\" %)\" )\n",
    "print(\"3 класс выжившие: \", class3_survived, \" from \",class3_all, \" ( \",class3_survived/class3_all*100,\" %)\" )\n",
    "\n",
    "# Вступившие на борт пассажиры\n",
    "survived_embarked=data[(data['Survived']==1)][\"Embarked\"]\n",
    "deceased_embarked=data[(data['Survived']==0)][\"Embarked\"]\n",
    "survived_embarked.hist(label=\"Выжившие\")\n",
    "deceased_embarked.hist(label=\"Умершие\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Пассажиры из разных портов\n",
    "pas_embarked_S=list(data.Embarked).count(\"S\")\n",
    "pas_embarked_C=list(data.Embarked).count(\"C\")\n",
    "pas_embarked_Q=list(data.Embarked).count(\"Q\")\n",
    "\n",
    "#Выжившие пассажиры в %\n",
    "percent_S=list(data.Embarked[data.Survived==1]).count(\"S\")/list(data.Embarked).count(\"S\")*100\n",
    "percent_C=list(data.Embarked[data.Survived==1]).count(\"C\")/list(data.Embarked).count(\"C\")*100\n",
    "percent_Q=list(data.Embarked[data.Survived==1]).count(\"Q\")/list(data.Embarked).count(\"Q\")*100\n",
    "\n",
    "# Результаты\n",
    "print(\" Отправление из порта 'S': \",pas_embarked_S, \" (\", percent_S, \"% выживших)\")\n",
    "print(\" Отправление из порта 'C': \",pas_embarked_C,\" (\", percent_C, \"% выживших)\")\n",
    "print(\" Отправление из порта 'Q': \",pas_embarked_Q, \" (\", percent_Q, \"% выживших)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выжили пассажиры в основном 1ого и 2ого классов, тк они были ближе к палубе погрузки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_missing_values=data.isna().sum()/data.isna().count()*100\n",
    "print(\"% пробелов в признаках : \",\"\\n\",data_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel, RFECV, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(os.path.join(PATH, 'titanic', 'test.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Survived\"]\n",
    "X =data.drop([\"Survived\",\"Ticket\",\"Cabin\",\"Name\",\"Fare\"], axis=1)\n",
    "X['Sex'] = X['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "X['Embarked']=X['Embarked'].fillna('Q')\n",
    "X['Embarked'] = X['Embarked'].map( {'S': 0, 'C': 1,'Q':2} ).astype(int)\n",
    "X['Age']=X['Age'].fillna(np.round(X['Age'].median(),0))\n",
    "\n",
    "#X_test\n",
    "X_test =data_test.drop([\"Ticket\",\"Cabin\",\"Name\",\"PassengerId\",\"Fare\"], axis=1)\n",
    "X_test['Sex'] = X_test['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "X_test['Embarked']=X_test['Embarked'].fillna('Q')\n",
    "X_test['Embarked'] = X_test['Embarked'].map( {'S': 0, 'C': 1,'Q':2} ).astype(int)\n",
    "X_test['Age']=X_test['Age'].fillna(np.round(X['Age'].median(),0))\n",
    "\n",
    "# StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#print(X_scaled)\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "#X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Y_train=y\n",
    "X_train_scaled=X_scaled\n",
    "logreg = LogisticRegressionCV(solver=\"lbfgs\",cv=5, penalty=\"l2\")\n",
    "logreg.fit(X_train_scaled, Y_train)\n",
    "Y_pred_log = logreg.predict(X_test_scaled)\n",
    "\n",
    "acc_log = round(logreg.score(X_train_scaled, Y_train) * 100, 2)\n",
    "print(\"Accuracy with LogReg (cv=5) = \",acc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# KNN with scaling data\n",
    "X_train=X_scaled\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred_knn = knn.predict(X_test_scaled)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "print(\"Accuracy with KNN = \",acc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": data_test[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred_log\n",
    "    })\n",
    "submission.to_csv('./submission_final_titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2 - Porto Seguro’s Safe Driver Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_data = pd.read_csv(os.path.join(PATH, 'porto', 'train.csv')).set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = insurance_data['target']\n",
    "X = insurance_data.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [column for column in X if not (column.endswith(\"cat\") or column.endswith(\"bin\"))]\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = [column for column in X if (column.endswith(\"cat\") or column.endswith(\"bin\"))]\n",
    "categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', SGDClassifier(loss='log', alpha=0.001, n_jobs=-1, random_state=14))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы получили результат модели - 0.96 Построим матрицу и проанализируем ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(clf, X_valid, y_valid,\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#К сожалению наша модель полностью бесполезна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target', data=insurance_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сalc gini coeff\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):  \n",
    "    assert(len(actual) == len(pred))  \n",
    "    epsilon = 1e-7\n",
    "    values = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)  \n",
    "    values = values[np.lexsort((values[:, 2], -1 * values[:, 1]))]  \n",
    "    total = values[:, 0].sum() \n",
    "    gini_sum = (values[:, 0].cumsum().sum() + epsilon) / (total + epsilon)  \n",
    "  \n",
    "    gini_sum -= (len(actual) + 1) / 2  \n",
    "    return gini_sum / len(actual)  \n",
    "  \n",
    "def gini_normalized(a, p):  \n",
    "    '''Function to calculate the normalized gini coefficient'''\n",
    "    return gini(a, p) / gini(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(X_valid)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_normalized(y_valid, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
